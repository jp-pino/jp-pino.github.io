<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Juan Pablo Pino</title>
    <link href="https://jp-pino.github.io/blog" />
    <link type="application/atom+xml" rel="self" href="https://jp-pino.github.io/blog/feed.atom" />
    <updated>2023-12-31T01:09:44+00:00</updated>
    <id>https://jp-pino.github.io/blog/feed.atom</id>
    <author>
        <name>Juan Pablo Pino</name>
    </author>
                <entry>
    <id>https://jp-pino.github.io/blog/mopiOS-new-features</id>
    <link type="text/html" rel="alternate" href="https://jp-pino.github.io/blog/mopiOS-new-features" />
    <title>New Features!</title>
    <published>2019-08-27T00:00:00+00:00</published>
    <updated>2019-08-27T00:00:00+00:00</updated>
    <author>
        <name>Juan Pablo Pino</name>
    </author>
    <summary type="html">Big news for mopiOS!!! There are many new features for semaphores, threads, debugging, and so much more! Among the new features you&#039;ll find:
* Threads are now named so it is easier to identify them.
* Memory for TCBs is now dynamically allocated on the......</summary>
    <content type="html"><![CDATA[
        <p>Big news for mopiOS!!! There are many new features for <em>semaphores</em>, <em>threads</em>, <em>debugging</em>, and so much more! Among the new features you'll find:
* Threads are now named so it is easier to identify them.
* Memory for TCBs is now dynamically allocated on the Heap.
* You can now print the jitter histogram directly from the CLI, just use <code>jitter -h</code>.
* Threads can now be killed from another thread through <code>killall</code> functionality.
* Semaphores now register on a list upon initialization, making it easier to find TCBs that are blocking inside them.
* The new command <code>ts</code> now counts the number of active threads. To list all threads with their names and status add the flag <code>-l</code>.</p>

<p>More is on the way! I'm currently working on a port of <strong>rosserial_client</strong> for mopiOS, to enable communication with the ROS platform. See the development on this port <a href="https://github.com/jp-pino/rosserial-mopiOS">here</a>.</p>
    ]]></content>
</entry>
            <entry>
    <id>https://jp-pino.github.io/blog/aes-file-encryption</id>
    <link type="text/html" rel="alternate" href="https://jp-pino.github.io/blog/aes-file-encryption" />
    <title>AES</title>
    <published>2019-08-18T00:00:00+00:00</published>
    <updated>2019-08-18T00:00:00+00:00</updated>
    <author>
        <name>Juan Pablo Pino</name>
    </author>
    <summary type="html">As a final project for the course &quot;Computer Architecture&quot;, Juan Manuel Ledesma and I designed a device that could cypher and decypher files using the Rijndael algorithm implementation of the Advanced Encryption Standard....</summary>
    <content type="html"><![CDATA[
        <p>As a final project for the course "Computer Architecture", <a href="#">Juan Manuel Ledesma</a> and I designed a device that could cypher and decypher files using the Rijndael algorithm implementation of the Advanced Encryption Standard.</p>
    ]]></content>
</entry>
            <entry>
    <id>https://jp-pino.github.io/blog/ieee-comments</id>
    <link type="text/html" rel="alternate" href="https://jp-pino.github.io/blog/ieee-comments" />
    <title>Image Processing</title>
    <published>2019-08-18T00:00:00+00:00</published>
    <updated>2019-08-18T00:00:00+00:00</updated>
    <author>
        <name>Juan Pablo Pino</name>
    </author>
    <summary type="html">Comments on &quot;Real-Time Panoramic Image Generation and Motion Deblurring by Using Dynamics-Based Robotic Vision&quot; by Michael Duckjune Kim &amp;amp; Jun Ueda from &quot;IEEE/ASME Transactions on Mechatronics Vol. 21&quot;

Out of all the papers I&#039;ve read for this......</summary>
    <content type="html"><![CDATA[
        <h3>Panoramic Images</h3>

<p><em>Comments on <a href="https://ieeexplore.ieee.org/document/7362211"><strong>"Real-Time Panoramic Image Generation and Motion Deblurring by Using Dynamics-Based Robotic Vision"</strong></a> by Michael Duckjune Kim &amp; Jun Ueda from <a href="https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=7463574">"IEEE/ASME Transactions on Mechatronics Vol. 21"</a></em></p>

<p>Out of all the papers I've read for this assignment, this one is the most related to robotics. The method this paper proposes for panoramic image stitching differs from the traditional methods in that the pictures are taken as the camera moves, instead of waiting to capture the image before the camera starts moving. The following diagram from the paper illustrates this:</p>

<blockquote>
  <p><img src="/assets/img/ieee-panoramic-diagram.png" alt="alt text" /></p>
  
  <p>Timeline to create a panoramic image from VGA images. The “move” block represents the settling time for a point-to-point motion using the robotic vision system. The “image” block represent the time of image acquisition. The “de” block represents the computation time of image deblurring that was set to 16.5 ms based on the results from Section IV-D.</p>
</blockquote>

<p>The proposed method is not only very time-efficient, but it also produces very good results. The actual implementation of this paper involves not only understanding of images and robotics, but interfacing between them too.</p>

<h3>Atmospheric Visibility Estimation</h3>

<p><em>Comments on <a href="https://ieeexplore.ieee.org/document/8412582"><strong>"Relative CNN-RNN: Learning Relative Atmospheric Visibility From Images"</strong></a> by Yang You &amp; Cewu Lu from <a href="https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=8468142&amp;punumber=83">"IEEE Transactions on Image Processing Vol. 28"</a></em></p>

<p>Reading this paper, it becomes clear that image processing is a field that may have far-reaching impacts on our lives. This paper proposes a method for estimating relative visibility from images using a relative CNN-RNN model. According to the paper, this model combines a Convolutional Neural Networks with a Recursive Neural Network which allows it to analyze the images with seven different attention regions in the images. As the authors mention, this approach produces good results in atmospheric visibility estimation and if implemented in a large-scale setup (e.g. using geo-tagged images uploaded to the web), it becomes much more affordable than the expensive scientific equipment used in very sparse weather observatories. <strong><em>Image processing</em></strong> and <strong><em>Robotic Vision</em></strong>, are clearly fields that offer new and innovative solutions to everyday problems today. However, when combined with other fields such as Data Science and Machine Learning, the solutions they offer become something out of a sci-fi movie.</p>

<h3>Object Counting</h3>

<p><em>Comments on <a href="https://ieeexplore.ieee.org/document/8488575"><strong>"Divide and Count: Generic Object Counting by Image Divisions"</strong></a> by Tobias Stahl, Silvia L. Pintea &amp; Jan C. van Gemert from <a href="https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=8478029&amp;punumber=83">"IEEE Transactions on Image Processing Vol. 28"</a></em></p>

<p>This paper proposes a method for generic object counting in images. From previous experiences, we can tell that objecting counting is essential for industrial applications, but as the paper mentions, emerging applications include fauna monitoring, traffic control, crowd management, etc. Similarly to the previous paper, the authors of this one use <strong><em>Data Science</em></strong> and <strong><em>Machine Learning</em></strong> to implement innovative solutions in the field of <strong><em>Image Processing</em></strong>. It becomes clear that these fields are becoming intertwined as research advances. The most common object counting applications however, have been constrained by the type of object that is being counted. There are car counting systems. There are fruit counting systems. There are pill counting systems. But all these systems work independently, are trained individually and only work for the specific object they were trained to count. I find the proposal in this paper appealing, because a system that can reliably count objects of any type, is more portable to other applications and can improve new and existing systems without investing too much effort on training.</p>

<h3>Underwater Image Enhancement</h3>

<p><em>Comments on <a href="https://ieeexplore.ieee.org/document/8058463"><strong>"Color Balance and Fusion for Underwater Image Enhancement"</strong></a> by Codruta O. Ancuti, Cosmin Ancuti, Christophe De Vleeschouwer, &amp; Philippe Bekaert from <a href="https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=8071125&amp;punumber=83">"IEEE Transactions on Image Processing Vol. 27"</a></em></p>

<p>I found this paper really interesting because the solution it proposes relates to an issue I (and many of us) have personally faced. Ever since the first waterproof cameras came out, underwater photography has faced many issues. Any camera can take a great picture on land, however light propagates differently underwater and thus distorts the shapes and colors in our images. Without special equipment or expensive hardware, the authors have developed a method to correct these distortions and generate an accurate image using only software. Their solution is pretty straightforward and, unlike the previous two papers, it has nothing to do with machine learning. It implements a pipeline of <em>only</em> image processing that produces amazing results.</p>

<h3>Image Deblurring</h3>

<p><em>Comments on <a href="https://ieeexplore.ieee.org/document/8488519"><strong>"Graph-Based Blind Image Deblurring From a Single Photograph"</strong></a> by Yuanchao Bai, Gene Cheung, Xianming Liu, &amp; Wen Gao from <a href="https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=8511005&amp;punumber=83">"IEEE Transactions on Image Processing Vol. 28"</a></em></p>

<p>This paper describes the method implemented by the authors to deblur images. I don't pretend to understand everything the paper explains in regard to their algorithm, but I get the base of it. In order to get the original image they iteratively estimate the blur kernel and use it to calculate a skeleton image, until both the blur kernel and the image no longer change. I found this paper very interesting and was impressed by the very realistic results they show. The images resulting from their algorithm clearly show an deblurred version of the original. This could have countless practical applications and I think is a very good example of what <em><strong>Image Processing</strong></em> entails: both understanding of how computers perceive images, and of other fields of math and science such as <em><strong>Probability</strong></em> and <em><strong>Linear Algebra</strong></em>.</p>
    ]]></content>
</entry>
            <entry>
    <id>https://jp-pino.github.io/blog/mopiOS</id>
    <link type="text/html" rel="alternate" href="https://jp-pino.github.io/blog/mopiOS" />
    <title>mopiOS</title>
    <published>2019-08-16T00:00:00+00:00</published>
    <updated>2019-08-16T00:00:00+00:00</updated>
    <author>
        <name>Juan Pablo Pino</name>
    </author>
    <summary type="html">As a final project for the course EE445M &quot;Embedded and Real-Time Operating Systems Laboratory&quot; at UT Austin, Cole Morgan and I started developing our own RTOS implementation. mopiOS is still a work in progress and I&#039;m currently working on porting it to......</summary>
    <content type="html"><![CDATA[
        <p>As a final project for the course EE445M "Embedded and Real-Time Operating Systems Laboratory" at UT Austin, <a href="https://github.com/coleamorgan">Cole Morgan</a> and I started developing our own RTOS implementation. <a href="https://github.com/jp-pino/mopiOS">mopiOS</a> is still a work in progress and I'm currently working on porting it to other microcontrollers and adding more functionality to it.</p>
    ]]></content>
</entry>
    </feed>
